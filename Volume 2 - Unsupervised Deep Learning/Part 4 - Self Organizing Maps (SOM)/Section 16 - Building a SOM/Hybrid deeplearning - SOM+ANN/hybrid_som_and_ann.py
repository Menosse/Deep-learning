# -*- coding: utf-8 -*-
"""Hybrid_SOM_AND_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eHslP9uikqlC9WDJxiUHE-zJQhoggL6u

#Self Organizing Map

##Install MiniSom Package
"""

!pip install MiniSom

from google.colab import drive
drive.mount('/content/drive')

"""### Importing the libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from minisom import MiniSom
from sklearn.preprocessing import MinMaxScaler
from pylab import bone, pcolor, colorbar, plot, show
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

"""## Importing the dataset"""

#dataset = pd.read_csv('Credit_Card_Applications.csv')
dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Hybrid/Credit_Card_Applications.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Feature Scaling"""

sc = MinMaxScaler(feature_range = (0,1))
X = sc.fit_transform(X)

"""##Training the SOM"""

som = MiniSom(x=10, y=10, input_len= len(X[0]), sigma= 1.0, learning_rate = 0.5)
som.random_weights_init(X)
som.train_random(data = X, num_iteration = 100)

"""##Visualizing the results"""

bone()
pcolor(som.distance_map().T)
colorbar()
markers = ['o', 's']
colors = ['r', 'g']
for i, x in enumerate(X):
    w = som.winner(x)
    plot(w[0] + 0.5,
         w[1] + 0.5,
         markers[y[i]],
         markeredgecolor = colors[y[i]],
         markerfacecolor = 'None',
         markersize = 10,
         markeredgewidth = 2)
show()

"""## Finding the frauds"""

mappings = som.win_map(X)
# The mappings below depends on the highest MID (Mean interneuron distance)
frauds = np.concatenate((mappings[(4,5)], mappings[(6,7)], mappings[(4,7)], mappings[(7,3)]), axis = 0)
frauds = sc.inverse_transform(frauds)

# matrix of features
customers = dataset.iloc[:, 1:].values

# Create dependent variables
is_fraud = np.zeros(len(dataset))
for i in range(len(dataset)):
  if dataset.iloc[i,0] in frauds:
    is_fraud[i] = 1

"""## Create ANN"""

sc = StandardScaler()
customers = sc.fit_transform(customers)
ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=1, activation="relu"))
ann.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))
ann.compile(optimizer="adam",loss="binary_crossentropy", metrics = ['accuracy'])

"""## Training ANN"""

ann.fit(customers, is_fraud, batch_size=1, epochs=30)

"""## Predicting test set results"""

y_pred = ann.predict(customers)
y_pred = np.concatenate((dataset.iloc[:, 0:1].values, y_pred), axis=1)
y_pred = y_pred[y_pred[:, 1].argsort()]

print(y_pred)